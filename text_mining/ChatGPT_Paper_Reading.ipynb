{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b562b141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import requests\n",
    "import PyPDF2\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import time\n",
    "from io import StringIO\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(text))\n",
    "    return num_tokens\n",
    "\n",
    "def get_txt_from_pdf(pdf_files,filter_ref = False, combine=False):\n",
    "    \"\"\"Convert pdf files to dataframe\"\"\"\n",
    "    # Create an empty list to store the data\n",
    "    data = []\n",
    "    # Iterate over the PDF\n",
    "    for pdf in pdf_files:\n",
    "        # Fetch the PDF content from the pdf\n",
    "        with open(pdf, 'rb') as pdf_content:\n",
    "            # Create a PDF reader object\n",
    "            pdf_reader = PyPDF2.PdfReader(pdf_content)\n",
    "            # Iterate over all the pages in the PDF\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page = pdf_reader.pages[page_num] # Extract the text from the current page\n",
    "                page_text = page.extract_text()\n",
    "                words = page_text.split() # Split the page text into individual words\n",
    "                page_text_join = ' '.join(words) # Join the words back together with a single space between each word\n",
    "\n",
    "                if filter_ref: #filter the reference at the end\n",
    "                    page_text_join = remove_ref(page_text_join)\n",
    "\n",
    "                page_len = len(page_text_join)\n",
    "                div_len = page_len // 4 # Divide the page into 4 parts\n",
    "                page_parts = [page_text_join[i*div_len:(i+1)*div_len] for i in range(4)]\n",
    "            \n",
    "                min_tokens = 40\n",
    "                for i, page_part in enumerate(page_parts):\n",
    "                    if count_tokens(page_part) > min_tokens:\n",
    "                        # Append the data to the list\n",
    "                        data.append({\n",
    "                            'file name': pdf,\n",
    "                            'page number': page_num + 1,\n",
    "                            'page section': i+1,\n",
    "                            'content': page_part,\n",
    "                            'tokens': count_tokens(page_part)\n",
    "                        })\n",
    "    # Create a DataFrame from the data\n",
    "    df = pd.DataFrame(data)\n",
    "    if combine:\n",
    "        df = combine_section(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_ref(pdf_text):\n",
    "    \"\"\"This function removes reference section from a given PDF text. It uses regular expressions to find the index of the words to be filtered out.\"\"\"\n",
    "    # Regular expression pattern for the words to be filtered out\n",
    "    pattern = r'(REFERENCES|Acknowledgment|ACKNOWLEDGMENT)'\n",
    "    match = re.search(pattern, pdf_text)\n",
    "\n",
    "    if match:\n",
    "        # If a match is found, remove everything after the match\n",
    "        start_index = match.start()\n",
    "        clean_text = pdf_text[:start_index].strip()\n",
    "    else:\n",
    "        # Define a list of regular expression patterns for references\n",
    "        reference_patterns = [\n",
    "            '\\[[\\d\\w]{1,3}\\].+?[\\d]{3,5}\\.','\\[[\\d\\w]{1,3}\\].+?[\\d]{3,5};','\\([\\d\\w]{1,3}\\).+?[\\d]{3,5}\\.','\\[[\\d\\w]{1,3}\\].+?[\\d]{3,5},',\n",
    "            '\\([\\d\\w]{1,3}\\).+?[\\d]{3,5},','\\[[\\d\\w]{1,3}\\].+?[\\d]{3,5}','[\\d\\w]{1,3}\\).+?[\\d]{3,5}\\.','[\\d\\w]{1,3}\\).+?[\\d]{3,5}',\n",
    "            '\\([\\d\\w]{1,3}\\).+?[\\d]{3,5}','^[\\w\\d,\\.– ;)-]+$',\n",
    "        ]\n",
    "\n",
    "        # Find and remove matches with the first eight patterns\n",
    "        for pattern in reference_patterns[:8]:\n",
    "            matches = re.findall(pattern, pdf_text, flags=re.S)\n",
    "            pdf_text = re.sub(pattern, '', pdf_text) if len(matches) > 500 and matches.count('.') < 2 and matches.count(',') < 2 and not matches[-1].isdigit() else pdf_text\n",
    "\n",
    "        # Split the text into lines\n",
    "        lines = pdf_text.split('\\n')\n",
    "\n",
    "        # Strip each line and remove matches with the last two patterns\n",
    "        for i, line in enumerate(lines):\n",
    "            lines[i] = line.strip()\n",
    "            for pattern in reference_patterns[7:]:\n",
    "                matches = re.findall(pattern, lines[i])\n",
    "                lines[i] = re.sub(pattern, '', lines[i]) if len(matches) > 500 and len(re.findall('\\d', matches)) < 8 and len(set(matches)) > 10 and matches.count(',') < 2 and len(matches) > 20 else lines[i]\n",
    "\n",
    "        # Join the lines back together, excluding any empty lines\n",
    "        clean_text = '\\n'.join([line for line in lines if line])\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "      \n",
    "def combine_section(df):\n",
    "    \"\"\"Merge sections, page numbers, add up content, and tokens based on the pdf name.\"\"\"\n",
    "    aggregated_df = df.groupby('file name').agg({\n",
    "        'content': aggregate_content,\n",
    "        'tokens': aggregate_tokens\n",
    "    }).reset_index()\n",
    "\n",
    "    return aggregated_df\n",
    "\n",
    "\n",
    "def aggregate_content(series):\n",
    "    \"\"\"Join all elements in the series with a space separator. \"\"\"\n",
    "    return ' '.join(series)\n",
    "\n",
    "\n",
    "def aggregate_tokens(series):\n",
    "    \"\"\"Sum all elements in the series.\"\"\"\n",
    "    return series.sum()\n",
    "\n",
    "\n",
    "def extract_title(file_name):\n",
    "    \"\"\"Extract the main part of the file name. \"\"\"\n",
    "    title = file_name.split('_')[0]\n",
    "    return title.rstrip('.pdf')\n",
    "\n",
    "\n",
    "def combine_main_SI(df):\n",
    "    \"\"\"Create a new column with the main part of the file name, group the DataFrame by the new column, \n",
    "    and aggregate the content and tokens.\"\"\"\n",
    "    df['main_part'] = df['file name'].apply(extract_title)\n",
    "    merged_df = df.groupby('main_part').agg({\n",
    "        'content': ''.join,\n",
    "        'tokens': sum\n",
    "    }).reset_index()\n",
    "\n",
    "    return merged_df.rename(columns={'main_part': 'file name'})\n",
    "\n",
    "\n",
    "def df_to_csv(df, file_name):\n",
    "    \"\"\"Write a DataFrame to a CSV file.\"\"\"\n",
    "    df.to_csv(file_name, index=False, escapechar='\\\\')\n",
    "\n",
    "\n",
    "def csv_to_df(file_name):\n",
    "    \"\"\"Read a CSV file into a DataFrame.\"\"\"\n",
    "    return pd.read_csv(file_name)\n",
    "\n",
    "\n",
    "\n",
    "def tabulate_condition(df,column_name):\n",
    "    \"\"\"This function converts the text from a ChatGPT conversation into a DataFrame.\n",
    "    It also cleans the DataFrame by dropping additional headers and empty lines.    \"\"\"\n",
    "    \n",
    "    table_text = df[column_name].str.cat(sep='\\n')\n",
    "\n",
    "    # Remove leading and trailing whitespace\n",
    "    table_text = table_text.strip()\n",
    "    \n",
    "    # Split the table into rows\n",
    "    rows = table_text.split('\\n')\n",
    "\n",
    "    # Extract the header row and the divider row\n",
    "    header_row, divider_row, *data_rows = rows\n",
    "\n",
    "    # Extract column names from the header row\n",
    "\n",
    "    column_names = ['compound name', 'metal source', 'metal amount', 'linker', 'linker amount',\n",
    "                   'modulator', 'modulator amount or volume', 'solvent', 'solvent volume', 'reaction temperature',\n",
    "                   'reaction time']\n",
    "\n",
    "    # Create a list of dictionaries to store the table data\n",
    "    data = []\n",
    "\n",
    "    # Process each data row\n",
    "    for row in data_rows:\n",
    "\n",
    "        # Split the row into columns\n",
    "        columns = [col.strip() for col in row.split('|') if col.strip()]\n",
    "    \n",
    "        # Create a dictionary to store the row data\n",
    "        row_data = {col_name: col_value for col_name, col_value in zip(column_names, columns)}\n",
    "    \n",
    "        # Append the dictionary to the data list\n",
    "        data.append(row_data)\n",
    "        \n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    \n",
    "    \"\"\"Make df clean by drop additional header and empty lines \"\"\"\n",
    "    def contains_pattern(s, patterns):\n",
    "        return any(re.search(p, s) for p in patterns)\n",
    "\n",
    "    def drop_rows_with_patterns(df, column_name):\n",
    "        #empty cells, N/A cells and header cells\n",
    "        patterns = [r'^\\s*$', r'--',r'-\\s-', r'compound', r'Compound',r'Compound name', r'Compound Name',\n",
    "                r'NaN',r'N/A',r'n/a',r'\\nN/A', r'note', r'Note']\n",
    "        \n",
    "        mask = df[column_name].apply(lambda x: not contains_pattern(str(x), patterns))\n",
    "        filtered_df = df[mask]\n",
    "    \n",
    "        return filtered_df\n",
    "    \n",
    "    \n",
    "    #drop the repeated header\n",
    "    df = drop_rows_with_patterns(df, 'compound name')\n",
    "    \n",
    "    #drop the organic synthesis (where the metal source is N/a)    \n",
    "    filtered_df = drop_rows_with_patterns(drop_rows_with_patterns(drop_rows_with_patterns(df,'metal source'),'metal amount'),'linker amount') \n",
    "\n",
    "    #drop the N/A rows\n",
    "    filtered_df = filtered_df.dropna(subset=['metal source','metal amount', 'linker amount'])\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "\n",
    "def split_content(input_string, tokens):\n",
    "    \"\"\"Splits a string into chunks based on a maximum token count. \"\"\"\n",
    "\n",
    "    MAX_TOKENS = tokens\n",
    "    split_strings = []\n",
    "    current_string = \"\"\n",
    "    tokens_so_far = 0\n",
    "\n",
    "    for word in input_string.split():\n",
    "        # Check if adding the next word would exceed the max token limit\n",
    "        if tokens_so_far + count_tokens(word) > MAX_TOKENS:\n",
    "            # If we've reached the max tokens, look for the last dot or newline in the current string\n",
    "            last_dot = current_string.rfind(\".\")\n",
    "            last_newline = current_string.rfind(\"\\n\")\n",
    "\n",
    "            # Find the index to cut the current string\n",
    "            cut_index = max(last_dot, last_newline)\n",
    "\n",
    "            # If there's no dot or newline, we'll just cut at the max tokens\n",
    "            if cut_index == -1:\n",
    "                cut_index = MAX_TOKENS\n",
    "\n",
    "            # Add the substring to the result list and reset the current string and tokens_so_far\n",
    "            split_strings.append(current_string[:cut_index + 1].strip())\n",
    "            current_string = current_string[cut_index + 1:].strip()\n",
    "            tokens_so_far = count_tokens(current_string)\n",
    "\n",
    "        # Add the current word to the current string and update the token count\n",
    "        current_string += \" \" + word\n",
    "        tokens_so_far += count_tokens(word)\n",
    "\n",
    "    # Add the remaining current string to the result list\n",
    "    split_strings.append(current_string.strip())\n",
    "\n",
    "    return split_strings\n",
    "\n",
    "\n",
    "def table_text_clean(text):\n",
    "    \"\"\"Cleans the table string and splits it into lines.\"\"\"\n",
    "\n",
    "    # Pattern to find table starts\n",
    "    pattern = r\"\\|\\s*compound\\s*.*\"\n",
    "\n",
    "    # Use re.finditer() to find all instances of the pattern in the string and their starting indexes\n",
    "    matches = [match.start() for match in re.finditer(pattern, text, flags=re.IGNORECASE)]\n",
    "\n",
    "    # Count the number of matches\n",
    "    num_matches = len(matches)\n",
    "\n",
    "    # Base table string\n",
    "    table_string = \"\"\"| compound name | metal source | metal amount | linker | linker amount | modulator | modulator amount or volume | solvent | solvent volume | reaction temperature | reaction time |\\n|---------------|-------|--------------|--------|---------------|-----------|---------------------------|---------|----------------|---------------------|---------------|\\n\"\"\"\n",
    "\n",
    "    if num_matches == 0:  # No table in the answer\n",
    "        print(\"No table found in the text: \" + text)\n",
    "        splited_text = ''\n",
    "\n",
    "    else:  # Split the text based on header\n",
    "        splited_text = ''\n",
    "        for i in range(num_matches):\n",
    "            # Get the relevant table slice\n",
    "            splited = text[matches[i]:matches[i + 1]] if i != (num_matches - 1) else text[matches[i]:]\n",
    "\n",
    "            # Remove the text after last '|'\n",
    "            last_pipe_index = splited.rfind('|')\n",
    "            splited = splited[:last_pipe_index + 1]\n",
    "\n",
    "            # Remove the header and \\------\\\n",
    "            pattern_dash = r\"-(\\s*)\\|\"\n",
    "            match = max(re.finditer(pattern_dash, splited), default=None, key=lambda x: x.start())\n",
    "\n",
    "            if not match:\n",
    "                print(\"'-|' pattern not found.\")\n",
    "            else:\n",
    "                first_pipe_index = match.start()\n",
    "                splited = '\\n' + splited[(first_pipe_index + len('-|\\n|') - 1):]  # Start from \"\\\"\n",
    "\n",
    "            splited_text += splited\n",
    "\n",
    "    table_string = table_string + splited_text\n",
    "    return table_string\n",
    "\n",
    "def add_similarity(df, given_embedding):\n",
    "    \"\"\"Adds a 'similarity' column to a dataframe based on cosine similarity with a given embedding.\"\"\"\n",
    "    def calculate_similarity(embedding):\n",
    "        # Check if embedding is a string and convert it to a list of floats if necessary\n",
    "        if isinstance(embedding, str):\n",
    "            embedding = [float(x) for x in embedding.strip('[]').split(',')]\n",
    "        return cosine_similarity([embedding], [given_embedding])[0][0]\n",
    "\n",
    "    df['similarity'] = df['embedding'].apply(calculate_similarity)\n",
    "    return df\n",
    "\n",
    "\n",
    "def select_top_neighbors(df):\n",
    "    \"\"\"Retains top-10 similarity sections and their neighbors in the dataframe and drops the rest.\"\"\"\n",
    "    # Sort dataframe by 'file name' and 'similarity' in descending order\n",
    "    df.sort_values(['file name', 'similarity'], ascending=[True, False], inplace=True)\n",
    "    \n",
    "    # Group dataframe by 'file name' and select the top 10 rows based on similarity\n",
    "    top_10 = df.groupby('file name').head(10)\n",
    "    \n",
    "    # Add neighboring rows (one above and one below) to the selection\n",
    "    neighbors = [i for index in top_10.index for i in (index - 1, index + 1) if 0 <= i < df.shape[0]]\n",
    "\n",
    "    # Create a new dataframe with only the selected rows\n",
    "    selected_df = df.loc[top_10.index.union(neighbors)]\n",
    "    return selected_df\n",
    "\n",
    "\n",
    "def add_emb(df):\n",
    "    \"\"\"Adds an 'embedding' column to a dataframe using OpenAI API.\"\"\"\n",
    "    openai.api_key = api_key\n",
    "    if 'embedding' in df.columns:\n",
    "        print('The dataframe already has embeddings. Please double check.')\n",
    "        return df\n",
    "\n",
    "    embed_msgs = []\n",
    "    for _, row in df.iterrows():\n",
    "        context = row['content']\n",
    "        context_emb = openai.Embedding.create(model=\"text-embedding-ada-002\", input=context)\n",
    "        embed_msgs.append(context_emb['data'][0]['embedding'])\n",
    "\n",
    "    df = df.copy()\n",
    "    df.loc[:, 'embedding'] = embed_msgs\n",
    "    \n",
    "    return df\n",
    "\n",
    "   \n",
    "\n",
    "def model_1(df):\n",
    "    \"\"\"Model 1 will turn text in dataframe to a summarized reaction condition table.The dataframe should have a column \"file name\" and a column \"exp content\".\"\"\"\n",
    "    response_msgs = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        column1_value = row[df.columns[0]]\n",
    "        column2_value = row['content']\n",
    "\n",
    "        max_tokens = 3000\n",
    "        if count_tokens(column2_value) > max_tokens:\n",
    "            context_list = split_content(column2_value, max_tokens)\n",
    "        else:\n",
    "            context_list = [column2_value]\n",
    "\n",
    "        answers = ''  # Collect answers from chatGPT\n",
    "        for context in context_list:\n",
    "            print(\"Start to analyze paper \" + str(column1_value) )\n",
    "            user_heading = f\"This is an experimental section on MOF synthesis from paper {column1_value}\\n\\nContext:\\n{context}\"\n",
    "            user_ending = \"\"\"Q: Can you summarize the following details in a table: \n",
    "            compound name or chemical formula (if the name is not provided), metal source, metal amount, organic linker(s), \n",
    "            linker amount, modulator, modulator amount or volume, solvent(s), solvent volume(s), reaction temperature, \n",
    "            and reaction time? If any information is not provided or you are unsure, use \"N/A\". \n",
    "            Please focus on extracting experimental conditions from only the MOF synthesis and ignore information related to organic linker synthesis, \n",
    "            MOF postsynthetic modification, high throughput (HT) experiment details or catalysis reactions. \n",
    "            If multiple conditions are provided for the same compound, use multiple rows to represent them. If multiple units or components are provided for the same factor (e.g.  g and mol for the weight, multiple linker or metals, multiple temperature and reaction time, mixed solvents, etc), include them in the same cell and separate by comma.\n",
    "            The table should have 11 columns, all in lowercase:\n",
    "            | compound name | metal source | metal amount | linker | linker amount | modulator | modulator amount or volume | solvent | solvent volume | reaction temperature | reaction time |\n",
    "\n",
    "            A:\"\"\"   \n",
    "\n",
    "            attempts = 3\n",
    "            while attempts > 0:\n",
    "                try:\n",
    "                    response = openai.ChatCompletion.create(\n",
    "                        model='gpt-3.5-turbo',\n",
    "                        messages=[{\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"\"\"Answer the question as truthfully as possible using the provided context,\n",
    "                                        and if the answer is not contained within the text below, say \"N/A\" \"\"\"\n",
    "                        },\n",
    "                            {\"role\": \"user\", \"content\": user_heading + user_ending}]\n",
    "                    )\n",
    "                    answer_str = response.choices[0].message.content\n",
    "                    if not answer_str.lower().startswith(\"n/a\"):\n",
    "                        answers += '\\n' + answer_str\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    attempts -= 1\n",
    "                    if attempts <= 0:\n",
    "                        print(f\"Error: Failed to process paper {column1_value}. Skipping. (model 1)\")\n",
    "                        break\n",
    "                    print(f\"Error: {str(e)}. Retrying in 60 seconds. {attempts} attempts remaining. (model 1)\")\n",
    "                    time.sleep(60)\n",
    "\n",
    "        response_msgs.append(answers)\n",
    "    df = df.copy()\n",
    "    df.loc[:, 'summarized'] = response_msgs\n",
    "    return df\n",
    "\n",
    "\n",
    "def model_2(df):\n",
    "    \"\"\"Model 2 has two parts. First, it asks ChatGPT to identify the experiment section,\n",
    "    then it combines the results\"\"\"\n",
    "    openai.api_key = api_key\n",
    "\n",
    "    response_msgs = []\n",
    "    \n",
    "    prev_paper_name = None  # Initialize the variable. For message printing purpose\n",
    "    total_pages = df.groupby(df.columns[0])[df.columns[1]].max() #  For message printing purpose\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        paper_name = row[df.columns[0]]\n",
    "        page_number = row[df.columns[1]]\n",
    "        # Only print the message when the paper name changes\n",
    "        if paper_name != prev_paper_name:\n",
    "            print(f'Processing paper: {paper_name}. Total pages: {total_pages[paper_name]}')\n",
    "            prev_paper_name = paper_name\n",
    "\n",
    "        context = row['content']\n",
    "\n",
    "        user_msg1 = \"\"\"\n",
    "        Context:\n",
    "        In a 4-mL scintillation vial, the linker H2PZVDC (91.0 mg, 0.5 mmol, 1 equiv.) was dissolved in N,N-dimethylformamide (DMF) (0.6 mL) upon sonication. An aqueous solution of AlCl3·6H2O (2.4 mL, 0.2 M, 1 equiv.) was added dropwise, and the resulting mixture was heated in a 120 °C oven for 24 hours.\n",
    "        Question: Does the section contain a comprehensive MOF synthesis with explicit reactant quantities or solvent volumes?\n",
    "        Answer: Yes.\n",
    "\n",
    "        Context:\n",
    "        These metal salt mixtures were combined with 18 mg of H4DOT and then dissolved in a mixture of DMF (6 mL), EtOH (0.36 mL), and water (0.36 mL). The reaction mixture was heated to specific temperatures (120°C or 85°C) for 24 h. \n",
    "        Question: Does the section contain a comprehensive MOF synthesis with explicit reactant quantities or solvent volumes?\n",
    "        Answer: Yes.\n",
    "\n",
    "        Context:\n",
    "        Synthesis of MOF-5.19 Zn(NO3)2?4H2O (31.824 mmol) and H2bdc (10.594 mmol) were dissolved in DEF (100 cm3). The solution was heated to 100 C.\n",
    "        Question: Does the section contain a comprehensive MOF synthesis with explicit reactant quantities or solvent volumes?\n",
    "        Answer: Yes.\n",
    "\n",
    "        Context:\n",
    "        [Zr6O4(OH)8(H2O)4(CTTA)8/3]·S (BUT-12·S). ZrCl4 (48 mg), H3CTTA (40 mg), and formic acid (8 mL) were ultrasonically dissolved in N,N′-dimethylformamide (DMF, 8 mL) in a 20 mL Pyrexvial.\n",
    "        Question: Does the section contain a comprehensive MOF synthesis with explicit reactant quantities or solvent volumes?\n",
    "        Answer: Yes.\n",
    "\n",
    "        Context:\n",
    "        A 0.150 M solution of imidazole in DMF and a 0.075M solution of Zn(NO3)2·4H2O in DMF were used as stock solutions, and heated in a 85 ºC isothermal oven for 3 days. \n",
    "        Question: Does the section contain a comprehensive MOF synthesis with explicit reactant quantities or solvent volumes?\n",
    "        Answer: Yes.\n",
    "        \n",
    "        Context:\n",
    "        Synthesis and Characterization of MOFs, Abbreviations, and General Procedures. For easy reference, the formulas for MOF-69-80, explanation of guest abbreviations and organic carboxylates, andcrystal unit cell parameters are listed in Table 2. Unless otherwiseindicated, chemicals were purchased from the Aldrich Chemical Co.and used as received. HPDC and ATC organic linkers were synthesizedaccording to published procedures.13 HPDC was protected, dehydro\u0002genated, and then deprotected to yield PDC.14\n",
    "        Question: Does the section contain a comprehensive MOF synthesis with explicit reactant quantities or solvent volumes?\n",
    "        Answer: No.\n",
    "\n",
    "        Context:\n",
    "        The design and synthesis of metal-organic frameworks (MOFs) has yielded a large number of structures which have been shown to have useful gas and liquid adsorption properties.1In particular, porous structures constructed from discrete metal\u0002carboxylate clusters and organic links have been shown to beamenable to systematic variation in pore size and functionality.\n",
    "        Question: Does the section contain a comprehensive MOF synthesis with explicit reactant quantities or solvent volumes?\n",
    "        Answer: No.\n",
    "        \n",
    "        Context:\n",
    "        Solvothermal reactions of Co(NO3)·6H2O, Hatz, and L1/L2 in a 2:2:1 molar ratio in DMF solvent at 180 °C for 24 h yielded\n",
    "        two crystalline products, 1 and 2, respectively\n",
    "        Question: Does the section contain a comprehensive MOF synthesis with explicit reactant quantities or solvent volumes?\n",
    "        Answer: No.\n",
    "        \n",
    "        Context:\n",
    "        A 22.9% weight loss was observed from 115 to 350 °C, which corresponds to the loss of one DEF molecule per formula unit (calcd: 23.5%).\n",
    "        Question: Does the section contain a comprehensive MOF synthesis with explicit reactant quantities or solvent volumes?\n",
    "        Answer: No.\n",
    "                \n",
    "        Context:\n",
    "          \"\"\"\n",
    "    \n",
    "        user_msg2 = \"\"\"\n",
    "        Question: Does the section contain a comprehensive MOF synthesis with explicit reactant quantities or solvent volumes?\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "        attempts = 3\n",
    "        while attempts > 0:\n",
    "            try:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model='gpt-3.5-turbo',\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"Determine if the section comes from an experimental section for MOF synthesis, which contains information on at least one of the following: reaction time, reaction temperature, metal source, organic linker, the amount, solvent and volume. Answer will be either Yes or No.\"},\n",
    "                        {\"role\": \"user\", \"content\": user_msg1 + context + user_msg2}\n",
    "                    ]\n",
    "                )\n",
    "                answers = response.choices[0].message.content\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                attempts -= 1\n",
    "                if attempts > 0:\n",
    "                    print(f\"Error: {str(e)}. Retrying in 60 seconds. {attempts} attempts remaining. (model 2)\")\n",
    "                    time.sleep(60)\n",
    "                else:\n",
    "                    print(f\"Error: Failed to process paper {paper_name}. Skipping. (model 2)\")\n",
    "                    answers = \"No\"\n",
    "                    break\n",
    "\n",
    "        response_msgs.append(answers)\n",
    "    df = df.copy()\n",
    "    df.loc[:,'classification'] = response_msgs\n",
    "\n",
    "\n",
    "    # The following section creates a new dataframe after applying some transformations to the old dataframe\n",
    "    # Create a boolean mask for rows where 'results' starts with 'No'\n",
    "    mask_no = df[\"classification\"].str.startswith(\"No\")\n",
    "    # Create a boolean mask for rows where both the row above and below have 'No' in the 'results' column\n",
    "    mask_surrounded_by_no = mask_no.shift(1, fill_value=False) & mask_no.shift(-1, fill_value=False)\n",
    "    # Combine the two masks with an AND operation\n",
    "    mask_to_remove = mask_no & mask_surrounded_by_no\n",
    "    # Invert the mask and filter the DataFrame\n",
    "    filtered_df = df[~mask_to_remove]\n",
    "    #combined\n",
    "    combined_df= combine_main_SI(combine_section(filtered_df ))\n",
    "    #call model 1 to summarized results\n",
    "    add_table_df = model_1(combined_df)\n",
    "    return add_table_df \n",
    "\n",
    "\n",
    "def model_3(df, prompt_choice=\"synthesis\", classfication = True):\n",
    "    \"\"\"Input a dataframe in broken separation, ~300 tokens, separated by pages and sections. This function will filter the unnecessary sections.\"\"\"\n",
    "\n",
    "    # Set up your API key\n",
    "    openai.api_key = api_key\n",
    "\n",
    "    # Define the prompt\n",
    "    prompts = {\n",
    "        \"synthesis\": \"Provide a detailed description of the experimental section or synthesis method used in this research. This section should cover essential information such as the compound name (e.g., MOF-5, ZIF-1, Cu(Bpdc), compound 1, etc.), metal source (e.g., ZrCl4, CuCl2, AlCl3, zinc nitrate, iron acetate, etc.), organic linker (e.g., terephthalate acid, H2BDC, H2PZDC, H4Por, etc.), amount (e.g., 25mg, 1.02g, 100mmol, 0.2mol, etc.), solvent (e.g., N,N Dimethylformamide, DMF, DCM, DEF, NMP, water, EtOH, etc.), solvent volume (e.g., 12mL, 100mL, 1L, 0.1mL, etc.), reaction temperature (e.g., 120°C, 293K, 100C, room temperature, reflux, etc.), and reaction time (e.g., 120h, 1 day, 1d, 1h, 0.5h, 30min, a week, etc.).\",\n",
    "        \"TGA\": \"\"\"Identify the section discussing thermogravimetric analysis (TGA) and thermal stability. This section typically includes information about weight-loss steps (e.g., 20%, 30%, 29.5%) and a decomposition temperature range (e.g., 450°C, 515°C) or a plateau.\"\"\",\n",
    "        \"sorption\": \"Identify the section discussing nitrogen (N2) sorption, argon sorption, Brunauer-Emmett-Teller (BET) surface area, Langmuir surface area, and porosity. This section typically reports values such as 1000 m2/g, 100 cm3/g STP, and includes pore diameter or pore size expressed in units of Ångströms (Å)\"\n",
    "    }\n",
    "        \n",
    "    #other than \"synthesis\", \"TGA\", \"sorption\"),the prompt choice can be the name of the linker to be searched for.\n",
    "    # If the choice is not one of the predefined ones (\"synthesis\", \"TGA\", \"sorption\"), it defaults to a generic prompt for the linker.\n",
    "    prompt = prompts.get(prompt_choice, f\"Provide the full name of linker ({prompt_choice}) or denoted as {prompt_choice} in chemicals, abstract, introduction or experimental section.\")\n",
    "    \n",
    "    # Create an embedding for the chosen prompt using OpenAI's embedding model\n",
    "    prompt_result = openai.Embedding.create(model=\"text-embedding-ada-002\", input=prompt)\n",
    "    # Extract the embedding data from the result\n",
    "    prompt_emb = prompt_result['data'][0]['embedding']\n",
    "\n",
    "    # If the dataframe does not already have an 'embedding' column, add one. This is done by calling the add_emb function on the dataframe\n",
    "    if 'embedding' not in df.columns:\n",
    "        df_with_emb = add_emb(df)\n",
    "    else:\n",
    "        df_with_emb  = df\n",
    "\n",
    "    # Add a 'similarity' column to the dataframe by comparing the embeddings.This is done by calling the add_similarity function on the dataframe and the prompt embedding\n",
    "    df_2 = add_similarity(df_with_emb, prompt_emb)\n",
    "\n",
    "    # Filter the dataframe to only include rows with top similarity and their neighbors\n",
    "    df_3 = select_top_neighbors(df_2)\n",
    "\n",
    "    # If the classification parameter is True, pass the dataframe to model_2 for further processing\n",
    "    if classfication:\n",
    "        return model_2(df_3)\n",
    "\n",
    "    # If the classification parameter is False, return the filtered dataframe as is\n",
    "    return df_3\n",
    "\n",
    "\n",
    "\n",
    "def load_paper(filename):\n",
    "    \"\"\"Crate a dataframe\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        dataframe = pd.read_csv(filename)\n",
    "        return dataframe\n",
    "    else:\n",
    "        #load pdf names\n",
    "        \n",
    "        with open('pdf_pool.csv', 'r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            pdf_pool = [row[0] for row in reader]\n",
    "        dataframe = get_txt_from_pdf(pdf_pool,combine = False, filter_ref = True)\n",
    "    \n",
    "        #store the dataframe\n",
    "        df_to_csv(dataframe, filename)\n",
    "\n",
    "        \n",
    "def load_paper_emb(filename):\n",
    "    \"\"\"Crate a dataframe that includes embedding information\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        paper_df_emb  = pd.read_csv(filename)\n",
    "        paper_df_emb['embedding'] = paper_df_emb['embedding'].apply(ast.literal_eval)\n",
    "        \n",
    "    else: #load paper and create embedding\n",
    "        paper_df_emb = add_emb(load_paper())\n",
    "    #store embedding to csv\n",
    "        df_to_csv(paper_df_emb, filename)\n",
    "    \n",
    "    return paper_df_emb\n",
    "\n",
    "\n",
    "def check_system(syn_df, paper_df, paper_df_emb):\n",
    "    \"\"\"Check if the data is correctly loaded\"\"\"\n",
    "    # check if openai.api_key is not placeholder\n",
    "    if openai.api_key  == \"Add Your OpenAI API KEY Here.\":\n",
    "        print(\"Error: Please replace openai.api_key with your actual key.\")\n",
    "        return False\n",
    "\n",
    "    # check if 'content' column exists in syn_df\n",
    "    if 'content' not in syn_df.columns:\n",
    "        print(\"Error: 'content' column is missing in syn_df.\")\n",
    "        return False\n",
    "\n",
    "    # check if 'paper_df' has at least four columns\n",
    "    expected_columns = ['file name', 'page number', 'page section', 'content']\n",
    "    if not all(col in paper_df.columns for col in expected_columns):\n",
    "        print(\"Error: 'paper_df' should have these columns: 'file name', 'page number', 'page section', 'content'.\")\n",
    "        return False\n",
    "\n",
    "    # check if 'embedding' column exists in paper_df_emb\n",
    "    if 'embedding' not in paper_df_emb.columns:\n",
    "        print(\"Error: 'embedding' column is missing in paper_df_emb.\")\n",
    "        return False\n",
    "\n",
    "    print(\"All checks passed.\")\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "#Load all dataframes\n",
    "openai.api_key = \"Add Your OpenAI API KEY Here.\"  #e.g. openai.api_key = \"abcdefg123abc\" \n",
    "syn_df = pd.read_csv(\"228paper_info.csv\")\n",
    "paper_df=load_paper(\"228paper_parsed.csv\")\n",
    "paper_df_emb = load_paper_emb(\"228paper_emb.csv\")\n",
    "check_system(syn_df, paper_df, paper_df_emb)\n",
    "\n",
    "#Run for Model 1\n",
    "model_1_table = tabulate_condition(model_1(syn_df),\"summarized\")\n",
    "\n",
    "#Run for Model 2\n",
    "model_2_table = tabulate_condition(model_2(paper_df),\"summarized\")\n",
    "\n",
    "#Run for Model 3\n",
    "model_3_table_2 = tabulate_condition( model_3(paper_df_emb),\"summarized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968f0b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
